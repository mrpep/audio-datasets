## Speech
### Speech recognition
* [**Common Voice**](https://voice.mozilla.org/en/datasets)
* [**Arabic speech corpus**](http://en.arabicspeechcorpus.com/)
* [**Spanish corpus**](http://www.ciempiess.org/downloads)
* [**Google Speech Commands**](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)
* [**VoxForge**](http://www.voxforge.org)

### Speech emotion recognition
* [**CREMA-D**](https://github.com/CheyneyComputerScience/CREMA-D)
* [**EMODB**](https://www.kaggle.com/piyushagni5/berlin-database-of-emotional-speech-emodb)
* [**eNTERFACE**](http://www.enterface.net/enterface05/docs/results/databases/project2_database.zip)
* [**IEMOCAP**](https://sail.usc.edu/iemocap/)
* [**KEIO-ESD**](http://research.nii.ac.jp/src/en/Keio-ESD.html)
* [**LSSED**](https://github.com/tobefans/LSSED)
* [**MELD**](https://affective-meld.github.io/)
* [**MSP-PODCAST**](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Podcast.html)
* [**RAVDESS**](https://zenodo.org/record/1188976#.X3-cZnVKiV4)
* [**RECOLA**](https://diuf.unifr.ch/main/diva/recola/download.html)
* [**SAVEE**](http://kahlan.eps.surrey.ac.uk/savee)
* [**TESS**](https://tspace.library.utoronto.ca/handle/1807/24487)

### Speech enhancement
* [**MS-SNSD**](https://github.com/microsoft/MS-SNSD#:~:text=Fork%2062-,The%20Microsoft%20Scalable%20Noisy%20Speech%20Dataset%20(MS%2DSNSD)%20is,Ratio%20(SNR)%20levels%20desired.)
* [**Device and Produced Speech Dataset (DAPS)**](https://zenodo.org/record/4660670#.YHN8cnVKiV4)
### Analysis
* [**Spanish Confusions Corpus**](http://laslab.org/resources/confusions/)
* [**Atlas interactivo de la entonación del español**](http://prosodia.upf.edu/atlasentonacion/presentacio/presentacio2.html)
* [**Hamburg Corpus of Argentinean Spanish (HaCASpa)**](https://corpora.uni-hamburg.de/hzsk/de/islandora/object/spoken-corpus:hacaspa#corpus-description)

### Speech Separation
* [**LibriMix**](https://github.com/JorisCos/LibriMix)

### Pathologies
* [**Saarbruecken Voice Database**](http://www.stimmdatenbank.coli.uni-saarland.de/help_en.php4)

### Speaker identification
* [**Voxceleb**](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/)

### Language identification
* https://www.kaggle.com/toponowicz/spoken-language-identification

## Music
### Cover identification
* [**Covers80**](https://labrosa.ee.columbia.edu/projects/coversongs/covers80/)
* [**Covers1000**](http://www.covers1000.net/)
* [**Mazurkas**](http://www.mazurka.org.uk/)
* [**Second Hand Songs**](https://secondhandsongs.com/)
* [**SHS100K**](https://github.com/NovaFrost/SHS100K)
* [**YouTube350**](https://sites.google.com/site/ismir2015shapelets/data)

### Tagging
* [**Magna tag a tune**](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)
* [**Melon Playlist Dataset**](https://mtg.github.io/melon-playlist-dataset)

### Annotation (MIDI+Audio)
* [**GuitarSet**](https://guitarset.weebly.com/)
* [**MAESTRO**](https://magenta.tensorflow.org/datasets/maestro)
* [**GROOVE**](https://magenta.tensorflow.org/datasets/groove)
* [**GiantMIDI-Piano**](https://github.com/bytedance/GiantMIDI-Piano)

### Source separation
* [**MUSDB18**](https://sigsep.github.io/datasets/musdb.html)
* [**DSD100**](https://sigsep.github.io/datasets/dsd100.html)
* [**Mixing Secrets**](https://www.cambridge-mt.com/ms/mtk/)

### Instruments
* [**NSYNTH**](https://magenta.tensorflow.org/nsynth)

### Instrument quality
* [**GoodSounds**](https://www.upf.edu/web/mtg/good-sounds)

### Tempo estimation
* [**GiantSteps**](https://github.com/GiantSteps/giantsteps-tempo-dataset)

### Songs/Analysis
* [**Free music archive (FMA)**](https://github.com/mdeff/fma)

## Environmental sounds
### Ambiences

* [**SoundPrivacy**](http://soundprivacy.aalto.fi/)

### Anomaly detection

* [**MIMII**](https://zenodo.org/record/3384388#.XtCGMnVKg5k)
* [**ToyADMOS**](https://zenodo.org/record/3351307#.XtCFynVKg5l)

### Room impulse responses

* [**BIRD**](https://github.com/FrancoisGrondin/BIRD)
* [**MIT IR Survey Database**](https://mcdermottlab.mit.edu/Reverb/IR_Survey.html)

### Noises

* [**DEMAND**](https://www.kaggle.com/aanhari/demand-dataset)
* [**QUT-NOISE**](https://github.com/qutsaivt/QUT-NOISE)
* [**WHAM!**](http://wham.whisper.ai/)

### Events
* [**Audioset**](https://research.google.com/audioset/download.html)
* [**FSDKaggle2019**](https://zenodo.org/record/3612637#.XyCUknVKjs0)
* [**FSD-50K**](https://zenodo.org/record/4060432#.X3t-kXVKiV4)
* [**DESED**](https://project.inria.fr/desed/)
* [**URBANSOUND-8K**](https://urbansounddataset.weebly.com/urbansound8k.html)
* [**ESC-50**](https://github.com/karolpiczak/ESC-50)
* [**ESC-US**](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YDEPUT)
* [**MUSAN**](https://www.openslr.org/17/) Differentiate music, speech and noise
* [**VGGSound**](https://www.robots.ox.ac.uk/~vgg/data/vggsound/)
* 

#### Mixtures
* [**Free Universal Sound Separation (FUSS)**](https://zenodo.org/record/3694384#.XxJP8XVKjs0)

### Scenes
#### Domestic
* [**SINS**](https://github.com/KULeuvenADVISE/SINS_database)
  - [Dev-set for DCASE2018-Task5](https://zenodo.org/record/1247102#.XxIhlHVKjs0)
  - [Eval-set for DCASE2018-Task5](https://zenodo.org/record/1964758)
* [**Epic-Kitchen 100**](https://epic-kitchens.github.io/2021)
  
### Birds
* **freefield1010**
  - [Audio](https://archive.org/download/ff1010bird/ff1010bird_wav.zip)
  - [Labels](https://ndownloader.figshare.com/files/10853303)
* **warblrb10k**
  - [Audio](https://archive.org/download/warblrb10k_public/warblrb10k_public_wav.zip)
  - [Labels](https://ndownloader.figshare.com/files/10853306)
* [**BirdVox Datasets**](https://wp.nyu.edu/birdvox/codedata/)
* [**DCASE2018-Task3 EvalSet**](https://zenodo.org/record/1298604#.XxIvs3VKjs0)

## Other
* [**Acoustic materials**](https://www.acousticsphere.com/acoustic-material-properties/?filters=insulation)

* [**Just Noticeable Differences**](http://percepaudio.cs.princeton.edu/icassp2020_perceptual/audio_perception.zip) Paper: https://arxiv.org/abs/2001.04460
